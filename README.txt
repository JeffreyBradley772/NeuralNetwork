Jeffrey Bradley
Prof. Watson
CSCI 315
3/4/2021

For Project 3, we are tasked with making a neural network that is trained using stochastic gradient descent and backpropagation that ultimately classifies the MNIST data set of images of hand drawn integers from 0 to 9. This represents the first important part of the 5 elements of the learning, the problem. 

For our data, we are given csv files of grayscale or intensity pixel values in 1D arrays per image where the first value in the array represents its actual label. This data represent the original 28x28 images that have been grayscaled and the array of pixel values flattened. The first part of the assignment is to get x_train, y_train, x_test, and y_test data. After splitting the data, so the y data represents the label and the x data represents the gray scale pixel values, I first made every pixel value orginally between 0 and 255 to be between 0.01 and 0.99. This was done to scale down the values to create a faster and more stable learning process when calculating error and adjusting weights that are initialized to low values. The y values were also adjusted so that each label is actually represented by an array of 10 values where the label represents the index with a value of 0.99 while every other index is a 0.01. This will ultimately allow us to calculate our neural networks output. These pieces represent another important element of learning, the data. Ultimately we finish with 60000 arrays of 784 scaled pixel values and 60000 label arrays for training and 10000 arrays of 784 scaled pixel values and 10000 label arrays for testing.

We are using this data to ultimately find the target function of the model which is the best set of weight matrices between layers of the neural network that will give us an accurate classification of the hand drawn image. The output of the trained model will give us a single integer representing the classification which is its final hypothesis.

Finally, there is the learning algorithm of the model. As stated, it uses stochastic gradient descent and back propogation to adjust the weights and achieve the target function. The model is represented in code by a python class and is initialized with a number of input nodes, hidden nodes, output nodes, and a learning rate. The intial model has 200 nodes per hidden layer, a learning rate of 0.01 and will have an input layer, two hidden layers, and an output layer. The number of input nodes will always be 784, for the 784 pixel values in the flattened 28x28 grayscaled image. The number of output nodes will always be 10 representing the 10 classifications of integers from 0 to 9. There are 3 weight matrices, one for input layer to the first hidden layer, one between the hidden layers, and one from the second hidden layer to the output layer, all of which are intiliazed to random low values pulled from a normal distribution. This random intitializiation of the weights as low values is important so that they can be more easily tuned during training. Since we are using stochastic gradient descent, error is calculated and the weights are adjusted after each single input. 

For each input array of scaled pixel values and correlated label array, we first pass the 784 input values from the input layer through the first hidden layer by dotting them with weight matrix representing the weights connecting the nodes of each layer. We then apply our activation function, the sigmoid function to get output values between 0 and 1 and pass it through the second hidden layer by again dotting it but with the next matrix representing the weights between the two hidden layers and applying the sigmoid activation function. Lastly, we complete the forward pass by taking the second hidden layers output, dotting it with the last matrix of weights between the last hidden layer and the output layer, and applying the activation function. At this point, the desire to use the sigmoid activation is realized as we finish with a array of 10 values where the value of each index represents the strength of that inputs correlation to an integer classification where index of the max value in the output array would be the models current classification. However, since it is still training, we take these output values between 0 and 1 and calculate its error by subtracting the correct label array. This is the beginning of back propogation where we use this output error to find each hidden layers error and adjust the weights accordingly. Each weight matrix is adjusted by moving down the gradient of the loss function where each gradient is calculated using the error from the output of the front layer, thus the output generated from that weight matrix. The key to this process is finding that error by propagating the output layers error backwards by dotting it with the each previous weight matrix. 

Once the initial model is created, the hyperparameters were tuned by comparing the mean squared error and accuracy of the models with different numbers of nodes per hidden layer, learning rates, and number of training epochs. The initial model with 200 hidden nodes, a 0.01 learning rate, and 5 epochs had a final MSE of 0.006 and a 96% accuracy. We then tried models with 100, 200, 300, 400, 500, and 600 hidden nodes and found that compotation time went up with each increase in the number of nodes. We ultimately found that using 200 hidden nodes gave us the best accuracy at 0.96 and 0.006 MSE. Then we changed the learning rate of the intial model to 0.001, 0.01, 0.1, 0.2, 0.4, and 0.6 and found that while the increasing the learning rate sped of training time, a 0.01 learning rate was most optimal as shown in the Accuracy and MSE figures for learning rate adjustments. Lastly, we adjusted the number of epochs or times the model went through every input image and adjusted the weights. As expected, more epochs significantly increased training time and better accuracy, however, increasing more than 10 epochs improved accuracy by less than 1%, so for our final model I chose to use 10 epochs for the sake of testing making our final hyperparameters 200 nodes per hidden layer, 0.01 learning rate, and 10 epochs. 

The final model with all the supposed best hyperparameters had an accuracy of 93% and loss of 0.005. Even though each accuracy was calculated with unseen test data. There is still evidence of overfitting, as when the model was tested on my own hand drawn gimp images, it only got 4 out of 10 of them right. This could be due to overfitting, the lack of a lot of data, or that the array of pixel values were in an irregular order than the images the model was originally trained with. In the last part of optimizing the model, each train image was rotated +10 and -10 degrees and added to the training set. After training the model with this new augmented data, the accuracy went down to about 89% but we can assume our model is much more generalizable. However, even with this augmented data, it still did poorly classifying my GIMP hand drawn images again implying overfitting or incorrect input data. 